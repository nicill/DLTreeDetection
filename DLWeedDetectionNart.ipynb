{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d7db48-a626-4bea-b962-309327a74638",
   "metadata": {},
   "source": [
    "# Weed Detection with Nart Data using Deep Learning\n",
    "\n",
    "In this notebook we will use several Deep Learning networks to detect Weeds of several species in RGB mosaics of Data Acquired in the NART research forest in Mongolia. We will consider the following species:  \n",
    "\n",
    "\n",
    "The data was collected by Sarangerel Jarantaibaatar and initial processing was carried on in collaboration with Shiful Islam.\n",
    "\n",
    "## Prerequisites.\n",
    "\n",
    "- Due to the size of the networks, this code will not run in free google colab. Either paid colab or running in locally in a computer with GPU is necessary. In the following I will assume that the code is run in a local computer that has a GPU and is CUDA-capable.\n",
    "- The computer needs to have software to run Python and jupyter notebook. `Anaconda` is recommended. If you are reading this, you most likely have already solved this part.\n",
    "- Apart from this, you need to create a proper virtual environment to run the code into. I recomment creating the environment with `Anaconda` itself and then installing packages using `pip`. At the very least you will need to install:\n",
    "    - opencv (for general image handling)\n",
    "    - pytorch (for general DL computations and the following models fasterRCNN, convnextMaskRCNN, maskRCNN, FCOS, retinanet, SSD  \n",
    "    - ultralytics for YOLO\n",
    "    - Transformers for DETR\n",
    "    - several other smaller libraries for several dependencies.\n",
    "\n",
    "If you are reading this here it is likely that you already have this, but you should have donwloaded all the code from [this repository](https://github.com/nicill/DLTreeDetection). This notebook along with all `.py` files. Download them using git or just go to the webpage, click `code` and then `download zip`.\n",
    "\n",
    "Once you have the folder with the code and this notebook, you should copy the data into it. The data is accessible [her](https://www.dropbox.com/scl/fi/b53k3eojxdrf0g0q7q305/NART.zip?rlkey=o8v9fwuqtle8l14w58awjqjmm&st=8lqk3op9&dl=0). Make a `Data` subfolder into the \n",
    "folder that contains the code and decompress the `NART.zip` file into it. You will end up with a Structure like Data->NART->test_image to access the data (there are four folders in the lower level: test_image, test_mask, train_image and train_mask.\n",
    "\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "Let's start by checking wheher or not the notebook has access to CUDA at this moment as it will determine whether or not you can run experiments on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05c6e88-b487-4214-966e-b2d9df2601e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x/anaconda3/envs/YOLO/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports \n",
    "import configparser\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from random import randint\n",
    "\n",
    "from datasets import TDDataset\n",
    "\n",
    "from config import read_config\n",
    "from imageUtils import boxesFound,read_Color_Image,read_Binary_Mask,recoupMasks, sliding_window, boxCoordsToFile\n",
    "from train import train_YOLO,makeTrainYAML, get_transform, train_pytorchModel,train_DETR, train_DeformableDETR\n",
    "\n",
    "from dataHandling import computeBBfromLIEnshurin, filterBoxesWindow, filterBoxesWindowNormalized\n",
    "\n",
    "from predict import predict_yolo, predict_pytorch\n",
    "\n",
    "from experimentsUtils import MODULARDLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2812dcf-6b7d-44f6-856f-e069cf801f49",
   "metadata": {},
   "source": [
    "You should be able to run the cell above without errors. If you get any `cannot import` errors it means that the environment in which you are running your notebook does not have all the necessary packages, install them using pip.\n",
    "\n",
    "Next, let's make sure that CUDA is accessible. CUDA is the library that manages the GPU and is absolutely crucial to get the code running in a time frame that allows experiments to be run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dacb6da-0a2f-4ec4-9f95-bf69357a5bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS CUDA AVAILABLE???????????????????????\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# checking that CUDA is available\n",
    "print(\"IS CUDA AVAILABLE???????????????????????\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d59c7e-5851-4f59-89b0-7e6d3f33d7ec",
   "metadata": {},
   "source": [
    "You should see the `IS CUDA AVAILABLE???????????????????????` message in the first line and `True` in the second. If the second line prints `False` then you do not have CUDA properly configured. Do that before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecc367-b3c2-40f6-9d36-4ec476b1bba3",
   "metadata": {},
   "source": [
    "# Data preprocessing:\n",
    "\n",
    "Before we can run any experiment we need to process the data into a format that Python and its Deep Learning library `Pytroch` can use. In particular, here we well divide every image into a series of tiles of the size that we decide. Additionally, we will translate all the image files into PNG format and the label images into text files with list of bounding box coordinates. The following two cells cell contains the cdoe that does all of this. The first function processes one image along with its label image and the second one runs it over all the files in a folder. As two of the DL networks that we use need slightly different data formats, the second function calls the first one twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "040d21b7-9838-434d-a09c-389edd610cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lIMfile, mosFile, outFolderRoot, trainPerc, doYOLO, slice, verbose = True):\n",
    "    \"\"\"\n",
    "        Given a mosaic and a label image, create bounding boxes for the label image\n",
    "        then tile mosaic and boxes and store in a folder\n",
    "        if trainPerc != 0 then divide into train and test folders\n",
    "    \"\"\"\n",
    "    labelIM = cv2.imread(lIMfile,cv2.IMREAD_UNCHANGED)\n",
    "    # check unique labels read    \n",
    "    #print(np.unique(labelIM))\n",
    "    #print(labelIM.shape)\n",
    "\n",
    "    mosaic = read_Color_Image(mosFile)\n",
    "    boxes, boxesNorm = computeBBfromLIEnshurin(labelIM)\n",
    "    # Visualize bounding boxes\n",
    "    #if verbose:\n",
    "    #    visualize_bounding_boxes(mosaic, boxes, num_slices=200, slice_size=(1024, 1024))\n",
    "\n",
    "    # If the train percentage is  0 then we are making one single folder \n",
    "    # otherwise, make one folder for trainining and one for validation\n",
    "    singleFolder = (trainPerc == 0)\n",
    "    \n",
    "    # create output folders if they do not exist, if mosaic mode create train andn test subfolders\n",
    "    Path(outFolderRoot).mkdir(parents=True, exist_ok=True)\n",
    "    if not singleFolder :\n",
    "        Path(os.path.join(outFolderRoot,\"train\")).mkdir(parents=True, exist_ok=True)\n",
    "        Path(os.path.join(outFolderRoot,\"validation\")).mkdir(parents=True, exist_ok=True)\n",
    "        if doYOLO:\n",
    "            Path(os.path.join(outFolderRoot,\"train\",\"images\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"train\",\"masks\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"train\",\"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"validation\",\"images\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"validation\",\"masks\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"validation\",\"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "    else:    \n",
    "        if doYOLO:\n",
    "            Path(os.path.join(outFolderRoot,\"images\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"masks\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # add the name of the original image to every tile as a prefix (without extension)\n",
    "    outputPrefix = os.path.basename(mosFile)[:-4] \n",
    "    \n",
    "    # slice the three things and output\n",
    "    wSize = (slice,slice)\n",
    "    count = 0\n",
    "    for (x, y, window) in sliding_window(mosaic, stepSize = int(slice*0.8), windowSize = wSize ):\n",
    "        # get mask window\n",
    "        if window.shape[:2] == (slice,slice) :\n",
    "            labelW = labelIM[y:y + wSize[1], x:x + wSize[0]]\n",
    "            boxesW = filterBoxesWindow(boxes,y,y + wSize[1], x,x + wSize[0])\n",
    "            boxesWNorm = boxesWNorm = filterBoxesWindowNormalized(boxesNorm, y, y+slice, x, x+slice, full_w=mosaic.shape[1], full_h=mosaic.shape[0])\n",
    "\n",
    "            if verbose: print(boxesW)\n",
    "\n",
    "            # here we should probably add cleanUpMaskBlackPixels and maybe do it for YOLO too (in buildtrainvalidation?)\n",
    "            if len(boxesW) > 0:\n",
    "                # store them both, doing a randomDraw to see if they go to training or testing\n",
    "                outFolder = (outFolderRoot if singleFolder else \n",
    "                (os.path.join(outFolderRoot,\"train\") if randint(1,100) < trainPerc else os.path.join(outFolderRoot,\"validation\") ) )\n",
    "                if verbose: print(\"writing to \"+str(os.path.join(outFolder,\"Tile\"+str(count)+\".png\")))\n",
    "                if doYOLO:\n",
    "                    cv2.imwrite(os.path.join(outFolder, \"images\",outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\".png\"),window)\n",
    "                    cv2.imwrite(os.path.join(outFolder,\"masks\",outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\"MASK.png\"),labelW)\n",
    "                    boxCoordsToFile(os.path.join(outFolder,\"labels\",outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\".txt\"),boxesWNorm)\n",
    "                else:                    \n",
    "                    cv2.imwrite(os.path.join(outFolder,outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\".png\"),window)\n",
    "                    cv2.imwrite(os.path.join(outFolder,outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\"Labels.tif\"),labelW)\n",
    "                    boxCoordsToFile(os.path.join(outFolder,outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\"Boxes.txt\"),boxesW)\n",
    "                count+=1\n",
    "            else:\n",
    "                if verbose: print(\"no boxes here\")\n",
    "        else:\n",
    "            if verbose:  print(\"sliceFolder, non full window, ignoring\"+str(window.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5649f947-3143-43b4-9039-7f4fac1ab877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliceAndBoxNartData(prefix, outputFolder, sliceSize = 500, trainPerc = 0):\n",
    "    \"\"\"\n",
    "        receive one folder divided into prefi_image and \n",
    "        prefix_mask, one slice size in pixels\n",
    "        traverse all images in the \"image\" folder\n",
    "        make sure that they have a corresponding mask.\n",
    "        Slice and box them all into an outputFolder\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(prefix+\"_image\"):\n",
    "        for imageFile in files:\n",
    "            labelImageFile = os.path.join(prefix+\"_mask\",imageFile)\n",
    "            if os.path.isfile( labelImageFile ):\n",
    "                prepareData(labelImageFile,os.path.join(prefix+\"_image\",imageFile),outputFolder, 0, False, sliceSize, verbose = False)\n",
    "                prepareData(labelImageFile,os.path.join(prefix+\"_image\",imageFile),outputFolder, trainPerc, True, sliceSize, verbose = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ce615-412d-4b57-8c1b-15d12e92aa76",
   "metadata": {},
   "source": [
    "To call these two functions we need to define the path to our Data. The values included in the cell suppose that the structure mentioned above is followed. Change if your path is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d2d844-f2f3-4802-a8e4-01d198aa1118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main data Folder is/home/x/Experiments/DLTreeDetection/Data/NART\n"
     ]
    }
   ],
   "source": [
    "# Define where our data is, change if necessary, this cell must be run in every execution!\n",
    "\n",
    "# Define main data folder, supposed to be within the directory that contains the notebook then Data then NART.\n",
    "# Depending on how you are running the jupyter notebook, you may need to change the definition of this folder\n",
    "dataFolder = os.path.join(os.getcwd(), \"Data\", \"NART\") \n",
    "print(\"Main data Folder is\" +str(dataFolder))\n",
    "\n",
    "# Define input and output Folders for the training and testing data\n",
    "inputTrain = os.path.join(dataFolder,\"train\")\n",
    "trainData = os.path.join(dataFolder,\"processedTrain\")\n",
    "\n",
    "inputTest = os.path.join(dataFolder,\"test\")\n",
    "testData = os.path.join(dataFolder,\"processedTest\")\n",
    "\n",
    "# Most of the parameters of our experiments will be defined later, but in this case, we need to define \n",
    "# the number of pixels that our tiles will have on the side (sliceSize)\n",
    "# and the percentage of tiles in the training/validation set that will be used for training \n",
    "sliceSize = 500\n",
    "trainPercentage = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e7a38-1949-4e46-9e2f-acbc6b822a6d",
   "metadata": {},
   "source": [
    "We are now ready to translate our Data to a format that Pytorch can read:\n",
    "\n",
    "**you only need to run the following cell once, if you do not change the data do not run this cell again** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4cefd22-065b-403d-878c-7d8081ee7b13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sliceAndBoxNartData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# convert all of our images to masks, box files and slices, if you did this before you do not need to run it again!\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# make one folder for training YOLO\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43msliceAndBoxNartData\u001b[49m(inputTrain,trainData, sliceSize \u001b[38;5;241m=\u001b[39m sliceSize, trainPerc \u001b[38;5;241m=\u001b[39m trainPercentage)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# also make one folder for testing\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sliceAndBoxNartData(inputTest,testData, sliceSize \u001b[38;5;241m=\u001b[39m sliceSize)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sliceAndBoxNartData' is not defined"
     ]
    }
   ],
   "source": [
    "# convert all of our images to masks, box files and slices, if you did this before you do not need to run it again!\n",
    "# make one folder for training YOLO\n",
    "sliceAndBoxNartData(inputTrain,trainData, sliceSize = sliceSize, trainPerc = trainPercentage)\n",
    "# also make one folder for testing\n",
    "sliceAndBoxNartData(inputTest,testData, sliceSize = sliceSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c97d9e-73a3-4b73-9b31-51a8b06d04f2",
   "metadata": {},
   "source": [
    "Check that two new folders have been created inside of the main data folder with the names indicated in `trainData` and `testData`.\n",
    "\n",
    "These two new folders should have:\n",
    "- Three subfolders called `images`, `labels` and `masks`\n",
    "- Lots of files corresponding to the tiles. Each Tile will have an image file (for example `patch_2x0y0.png`) a text file with the information of its bounding boxes (for example `patch_2x0y0Boxes.txt`) and a labelimage (for example patch_2x0y0Labels.tif).\n",
    "\n",
    "Check that all of this worked without errors and we are now ready to run our...\n",
    "\n",
    "# Experiments\n",
    "\n",
    "These experiments are pretty complex and allow to train and test quite a lot of different networks. To begin with I focused only on the two networks that are more likely to give the best results. YOLO and MaskRCNN.\n",
    "\n",
    "Interesting Parameters:\n",
    "\n",
    "- \"Train\": Determines whether or not we want to retrain our model. Keep it `True` initially but if you only want to re-test your model, then you can set it to `False`.\n",
    "- \"ep\" contains the number of epochs that the model will be trained for. I recommend running an initial run with one epoch to test if your model works and then set it to whatever value you consider correct. So far a value of 200 is pretty commonly used.\n",
    "- \"batchSize\" is set to 4. This parameter is crucial in terms of speed, the bigger the better. However, if you make it too big then the GPU will run out of memory and the code will crash. I used size 8 in my large computer and every epoch took less than a minute. with size 1 every epoch was about 4 minutes. I recommend testing this extensively before launching into long experiments. The final stages of the code use a bit of extra GPU memory so do not cut it too short.  \n",
    "- \"doYolo\" and \"doPytorch\" control what network(s) we use. I recommend starting with the one-epoch experiment for both YOLO and Pytorch and then setting up individual experiment setting the other value to `False`. For the moment leave the \"doDETR\" parameter at `False`. This is another network that we can compare if we are interested in making a large comparison of models but the code has not been adapted properly and it is not likely to give better results.\n",
    "- the \"numClasses\" parameter is set to `3` and should reflect the number of different weed species present in the label data. If the number is correct you do not need to ever change it.\n",
    "- The \"Train_Perc\" and \"slice\" parameters refer to two previous parameters defined when we created the training data. If you want to change them, go back there, change them there and make sure to re-run the function to create the data (also erase the data previously processed to be sure). Do not change them here.\n",
    "- the    \"Pred_dir\" and \"outTEXT\" parameters contain the route to the output that the algorithm creates. The output for the Yolo algorithm is also stored in \"Train_res\" and \"Valid_res\". Change them if you prefer. **Make sure to check these out as they contain quite a lot of interesting information.** In particular, Yolo will dump lots of tile prediction files into the root of the predictions folder. MaskRCNN will create one subfolder for each parameter configuration. Inside of each subfolder you will find lots of files related to tile and one more subfolder called \"full\". Check this \"full\" folder for the masks of every full image and a \"Pretty\" image that shows the prediction boxes superimposed to the original image along with the category of each one. At this moment Yolo does not create these \"Pretty\" images, it is on the TODO list.\n",
    "- The code produces precision and recall values. **These are detection only.** so they do not concern themselves with whether the weed inside the box is very well segmented or not, only that it is properly found. Also, at this moment they do not check the category of the weed. For example, if a weed is detected correctly (it box is properly placed) but the category is wrong (1 is predicted instead of 2), at this moment this will be counted as a correct prediction. This will be improved in the near future, hopefully.\n",
    "  \n",
    "The rest of the parameters can be safely ignore for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba310360-8c43-4e57-8836-4dfb06863cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration of our experiments\n",
    "conf = {\n",
    "    \"Train\" : True,\n",
    "    \"ep\" : 1,\n",
    "    \"batchSize\": 4,\n",
    "    \"doYolo\" : False,\n",
    "    \"doPytorch\" : True,\n",
    "    \"doDETR\" : False,\n",
    "    \"numClasses\" : 3,\n",
    "    \"Train_Perc\" : trainPercentage,\n",
    "    \"slice\": sliceSize,\n",
    "    \"Pred_dir\" : \"Predictions\",\n",
    "    \"outTEXT\": \"results.txt\",\n",
    "    \"Train_res\": os.path.join(os.getcwd(), \"YOLOResults\"),\n",
    "    \"Valid_res\": os.path.join(os.getcwd(), \"YOLOResults\"),\n",
    "    \"Prep\" : False,\n",
    "    \"TV_dir\" : os.path.join(dataFolder,\"processedTrain\"),\n",
    "    \"Train_dir\" : \"train\",\n",
    "    \"Valid_dir\" : \"validation\",\n",
    "    \"Test_dir\" : os.path.join(dataFolder,\"processedTest\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c94ca-fdab-4533-9ae3-704854d555e4",
   "metadata": {},
   "source": [
    "Finally, the cell that actually runs the experiments. Let's start by leaving it as it is and then we can complicate things a bit by considering different parameters, other pytorch models, the DETR model. For now, let's make sure that it all works properly and that it can be run in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e024c811-e6ba-4d18-aad7-53a8401b4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n",
      "\n",
      "=== Running YOLO Experiment ===\n",
      "Parameters: {'scale': 0.3, 'mosaic': 0.5}\n",
      "doing this \n",
      "YAMLtrainEXP.yaml\n",
      "New https://pypi.org/project/ultralytics/8.3.237 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.26 üöÄ Python-3.11.9 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Ti Laptop GPU, 15992MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=trainEXP.yaml, epochs=1, time=None, patience=10, batch=16, imgsz=500, save=True, save_period=-1, cache=False, device=0, workers=8, project=None, name=expscale0.3mosaic0.5epochs1ex, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.3, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=0.5, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/home/x/Experiments/DLTreeDetection/YOLOResults/detect/expscale0.3mosaic0.5epochs1ex\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011433 parameters, 3011417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "WARNING ‚ö†Ô∏è imgsz=[500] must be multiple of max stride 32, updating to [512]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/x/Experiments/DLTreeDetection/Data/NART/processedTrain/train/labels... 428 images, 429 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 856/856 [00:00<00:00, 1580.42it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/x/Experiments/DLTreeDetection/Data/NART/processedTrain/train/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/x/Experiments/DLTreeDetection/Data/NART/processedTrain/validation/labels... 101 images, 101 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:00<00:00, 851.49it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/x/Experiments/DLTreeDetection/Data/NART/processedTrain/validation/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/x/Experiments/DLTreeDetection/YOLOResults/detect/expscale0.3mosaic0.5epochs1ex/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 512 train, 512 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/x/Experiments/DLTreeDetection/YOLOResults/detect/expscale0.3mosaic0.5epochs1ex\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1      0.35G      1.969      3.033      1.439         75        512: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 54/54 [00:08<00:00,  6.53it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:02<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202       1248     0.0155      0.531     0.0691     0.0308\n",
      "\n",
      "1 epochs completed in 0.003 hours.\n",
      "Optimizer stripped from /home/x/Experiments/DLTreeDetection/YOLOResults/detect/expscale0.3mosaic0.5epochs1ex/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /home/x/Experiments/DLTreeDetection/YOLOResults/detect/expscale0.3mosaic0.5epochs1ex/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /home/x/Experiments/DLTreeDetection/YOLOResults/detect/expscale0.3mosaic0.5epochs1ex/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.26 üöÄ Python-3.11.9 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Ti Laptop GPU, 15992MiB)\n",
      "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202       1248     0.0155       0.53     0.0693     0.0308\n",
      "                   sp1         94        919     0.0398      0.606      0.125     0.0545\n",
      "                   sp2         49        199    0.00361      0.407      0.031     0.0151\n",
      "                   sp3         37        130     0.0031      0.577     0.0515     0.0227\n",
      "Speed: 0.2ms preprocess, 1.0ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/home/x/Experiments/DLTreeDetection/YOLOResults/detect/expscale0.3mosaic0.5epochs1ex\u001b[0m\n",
      "Ultralytics YOLOv8.2.26 üöÄ Python-3.11.9 torch-2.3.0+cu121 CUDA:0 (NVIDIA GeForce RTX 3080 Ti Laptop GPU, 15992MiB)\n",
      "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/x/Experiments/DLTreeDetection/Data/NART/processedTrain/validation/labels.cache... 101 images, 101 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 202/202 [00:00<?, ?it/s]\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:05<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        202       1248     0.0155      0.527     0.0692     0.0307\n",
      "                   sp1         94        919     0.0399      0.605      0.126     0.0545\n",
      "                   sp2         49        199    0.00361      0.407     0.0307      0.015\n",
      "                   sp3         37        130    0.00305      0.569     0.0514     0.0228\n",
      "Speed: 0.2ms preprocess, 11.5ms inference, 0.0ms loss, 2.5ms postprocess per image\n",
      "Saving /home/x/Experiments/DLTreeDetection/YOLOResults/predictions.json...\n",
      "Results saved to \u001b[1m/home/x/Experiments/DLTreeDetection/YOLOResults\u001b[0m\n",
      "Model saved to: /home/x/Experiments/DLTreeDetection/YOLOResults/detect/expscale0.3mosaic0.5epochs1ex/weights/best.pt\n",
      "Testing YOLO model...\n",
      "/home/x/Experiments/DLTreeDetection/Data/NART/processedTest/images\n",
      "predict_yolo: Loading model from /home/x/Experiments/DLTreeDetection/YOLOResults/detect/expscale0.3mosaic0.5epochs1ex/weights/best.pt\n",
      "average Precision (centroids) 0.0\n",
      "average Recall (centroids) 0.0\n",
      "global Precision (overlap) 0\n",
      "global Recall (overlap) 0.0\n",
      "YOLO Results: Precision=0.000, Recall=0.000\n",
      "\n",
      "=== Running PyTorch Model Experiment ===\n",
      "Parameters: {'modelType': 'maskrcnn', 'score': 0.25, 'nms': 0.5, 'predconf': 0.7}\n",
      "Train dataset length: 529\n",
      "Testing params {'modelType': 'maskrcnn', 'score': 0.25, 'nms': 0.5, 'predconf': 0.7} with file expmodelTypemaskrcnnscore0.25nms0.5Epochs1.pth\n",
      "Inside Pytorch training Training Dataset Length 529\n",
      "not training again\n",
      "Testing Dataset Length 124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m detr_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelType\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDETR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5e-6\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m8\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredconf\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \n\u001b[1;32m      4\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnms_iou\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_detections\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m800\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoDETR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run experiments\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mMODULARDLExperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpytorch_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetr_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Experiments/DLTreeDetection/experimentsUtils.py:260\u001b[0m, in \u001b[0;36mMODULARDLExperiment\u001b[0;34m(conf, yolo_params, pytorch_params, detr_params)\u001b[0m\n\u001b[1;32m    258\u001b[0m experiment \u001b[38;5;241m=\u001b[39m PyTorchModelExperiment(conf, device)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m metrics, train_time, test_time \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpytorch_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m experiment\u001b[38;5;241m.\u001b[39mwrite_results(pytorch_params, metrics, train_time, test_time)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch Results: Precision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moprec\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morec\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Experiments/DLTreeDetection/experimentsUtils.py:136\u001b[0m, in \u001b[0;36mPyTorchModelExperiment.train_and_test\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    133\u001b[0m pred_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPred_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m paramsDictToString(params))\n\u001b[1;32m    134\u001b[0m orig_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTV_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m prec, rec, oprec, orec \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_pytorch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredConfidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredconf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpostProcess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredFolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43morigFolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_folder\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m test_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    147\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprec\u001b[39m\u001b[38;5;124m'\u001b[39m: prec, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrec\u001b[39m\u001b[38;5;124m'\u001b[39m: rec, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moprec\u001b[39m\u001b[38;5;124m'\u001b[39m: oprec, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morec\u001b[39m\u001b[38;5;124m'\u001b[39m: orec}\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Experiments/DLTreeDetection/predict.py:865\u001b[0m, in \u001b[0;36mpredict_pytorch\u001b[0;34m(dataset_test, model, device, predConfidence, postProcess, predFolder, origFolder)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# store test images to disk\u001b[39;00m\n\u001b[1;32m    863\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images)\n\u001b[0;32m--> 865\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    866\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m    868\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(cpu_device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torchvision/models/detection/rpn.py:371\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    367\u001b[0m objectness, pred_bbox_deltas \u001b[38;5;241m=\u001b[39m concat_box_prediction_layers(objectness, pred_bbox_deltas)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# apply pred_bbox_deltas to anchors to obtain the decoded proposals\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# the proposals\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_coder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_bbox_deltas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m proposals \u001b[38;5;241m=\u001b[39m proposals\u001b[38;5;241m.\u001b[39mview(num_images, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    373\u001b[0m boxes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_proposals(proposals, objectness, images\u001b[38;5;241m.\u001b[39mimage_sizes, num_anchors_per_level)\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torchvision/models/detection/_utils.py:178\u001b[0m, in \u001b[0;36mBoxCoder.decode\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    177\u001b[0m     rel_codes \u001b[38;5;241m=\u001b[39m rel_codes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m pred_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box_sum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    180\u001b[0m     pred_boxes \u001b[38;5;241m=\u001b[39m pred_boxes\u001b[38;5;241m.\u001b[39mreshape(box_sum, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torchvision/models/detection/_utils.py:216\u001b[0m, in \u001b[0;36mBoxCoder.decode_single\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    213\u001b[0m pred_h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(dh) \u001b[38;5;241m*\u001b[39m heights[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Distance from center to box's corner.\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m c_to_c_h \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_ctr_y\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_h\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m pred_h\n\u001b[1;32m    217\u001b[0m c_to_c_w \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mpred_ctr_x\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mpred_w\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m*\u001b[39m pred_w\n\u001b[1;32m    219\u001b[0m pred_boxes1 \u001b[38;5;241m=\u001b[39m pred_ctr_x \u001b[38;5;241m-\u001b[39m c_to_c_w\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "yolo_params = {\"scale\": 0.3, \"mosaic\": 0.5} if conf[\"doYolo\"] else None\n",
    "pytorch_params = {\"modelType\": \"maskrcnn\", \"score\": 0.25, \"nms\": 0.5, \"predconf\": 0.7} if conf[\"doPytorch\"] else None\n",
    "detr_params = {\"modelType\": \"DETR\", \"lr\": 5e-6, \"batch_size\": 8, \"predconf\": 0.5, \n",
    "               \"nms_iou\": 0.5, \"max_detections\": 50, \"resize\": 800} if conf[\"doDETR\"] else None\n",
    "\n",
    "# Run experiments\n",
    "MODULARDLExperiment(conf, yolo_params, pytorch_params, detr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bfe1b5-2c6c-467d-9931-3f28155c08ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee4d12-8e8f-47ed-a02c-fbc4d416efcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
