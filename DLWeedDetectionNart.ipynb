{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d7db48-a626-4bea-b962-309327a74638",
   "metadata": {},
   "source": [
    "# Weed Detection with Nart Data using Deep Learning\n",
    "\n",
    "In this notebook we will use several Deep Learning networks to detect Weeds of several species in RGB mosaics of Data Acquired in the NART research forest in Mongolia. We will consider the following species:  \n",
    "\n",
    "\n",
    "The data was collected by Sarangerel Jarantaibaatar and initial processing was carried on in collaboration with Shiful Islam.\n",
    "\n",
    "## Prerequisites.\n",
    "\n",
    "- Due to the size of the networks, this code will not run in free google colab. Either paid colab or running in locally in a computer with GPU is necessary. In the following I will assume that the code is run in a local computer that has a GPU and is CUDA-capable.\n",
    "- The computer needs to have software to run Python and jupyter notebook. `Anaconda` is recommended. If you are reading this, you most likely have already solved this part.\n",
    "- Apart from this, you need to create a proper virtual environment to run the code into. I recomment creating the environment with `Anaconda` itself and then installing packages using `pip`. At the very least you will need to install:\n",
    "    - opencv (for general image handling)\n",
    "    - pytorch (for general DL computations and the following models fasterRCNN, convnextMaskRCNN, maskRCNN, FCOS, retinanet, SSD  \n",
    "    - ultralytics for YOLO\n",
    "    - Transformers for DETR\n",
    "    - several other smaller libraries for several dependencies.\n",
    "\n",
    "If you are reading this here it is likely that you already have this, but you should have donwloaded all the code from [this repository](https://github.com/nicill/DLTreeDetection). This notebook along with all `.py` files. Download them using git or just go to the webpage, click `code` and then `download zip`.\n",
    "\n",
    "Once you have the folder with the code and this notebook, you should copy the data into it. The data is accessible [her](https://www.dropbox.com/scl/fi/b53k3eojxdrf0g0q7q305/NART.zip?rlkey=o8v9fwuqtle8l14w58awjqjmm&st=8lqk3op9&dl=0). Make a `Data` subfolder into the \n",
    "folder that contains the code and decompress the `NART.zip` file into it. You will end up with a Structure like Data->NART->test_image to access the data (there are four folders in the lower level: test_image, test_mask, train_image and train_mask.\n",
    "\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "Let's start by checking wheher or not the notebook has access to CUDA at this moment as it will determine whether or not you can run experiments on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c04906b-5ce1-4405-942b-8a67437bc1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not actually needed, debugging purposes\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d05c6e88-b487-4214-966e-b2d9df2601e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports \n",
    "import configparser\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from random import randint\n",
    "\n",
    "from datasets import TDDataset\n",
    "\n",
    "from config import read_config\n",
    "from imageUtils import boxesFound,read_Color_Image,read_Binary_Mask,recoupMasks, sliding_window, boxCoordsToFile\n",
    "from train import train_YOLO,makeTrainYAML, get_transform, train_pytorchModel,train_DETR\n",
    "\n",
    "from dataHandling import computeBBfromLIEnshurin, filterBoxesWindow, filterBoxesWindowNormalized\n",
    "\n",
    "from predict import predict_yolo, predict_pytorch\n",
    "\n",
    "from experimentsUtils import MODULARDLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2812dcf-6b7d-44f6-856f-e069cf801f49",
   "metadata": {},
   "source": [
    "You should be able to run the cell above without errors. If you get any `cannot import` errors it means that the environment in which you are running your notebook does not have all the necessary packages, install them using pip.\n",
    "\n",
    "Next, let's make sure that CUDA is accessible. CUDA is the library that manages the GPU and is absolutely crucial to get the code running in a time frame that allows experiments to be run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dacb6da-0a2f-4ec4-9f95-bf69357a5bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS CUDA AVAILABLE???????????????????????\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# checking that CUDA is available\n",
    "print(\"IS CUDA AVAILABLE???????????????????????\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d59c7e-5851-4f59-89b0-7e6d3f33d7ec",
   "metadata": {},
   "source": [
    "You should see the `IS CUDA AVAILABLE???????????????????????` message in the first line and `True` in the second. If the second line prints `False` then you do not have CUDA properly configured. Do that before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecc367-b3c2-40f6-9d36-4ec476b1bba3",
   "metadata": {},
   "source": [
    "# Data preprocessing:\n",
    "\n",
    "Before we can run any experiment we need to process the data into a format that Python and its Deep Learning library `Pytroch` can use. In particular, here we well divide every image into a series of tiles of the size that we decide. Additionally, we will translate all the image files into PNG format and the label images into text files with list of bounding box coordinates. The following two cells cell contains the cdoe that does all of this. The first function processes one image along with its label image and the second one runs it over all the files in a folder. As two of the DL networks that we use need slightly different data formats, the second function calls the first one twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "040d21b7-9838-434d-a09c-389edd610cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lIMfile, mosFile, outFolderRoot, trainPerc, doYOLO, slice, verbose = True):\n",
    "    \"\"\"\n",
    "        Given a mosaic and a label image, create bounding boxes for the label image\n",
    "        then tile mosaic and boxes and store in a folder\n",
    "        if trainPerc != 0 then divide into train and test folders\n",
    "    \"\"\"\n",
    "    labelIM = cv2.imread(lIMfile,cv2.IMREAD_UNCHANGED)\n",
    "    # check unique labels read    \n",
    "    #print(np.unique(labelIM))\n",
    "    #print(labelIM.shape)\n",
    "\n",
    "    mosaic = read_Color_Image(mosFile)\n",
    "    boxes, boxesNorm = computeBBfromLIEnshurin(labelIM)\n",
    "    # Visualize bounding boxes\n",
    "    #if verbose:\n",
    "    #    visualize_bounding_boxes(mosaic, boxes, num_slices=200, slice_size=(1024, 1024))\n",
    "\n",
    "    # If the train percentage is  0 then we are making one single folder \n",
    "    # otherwise, make one folder for trainining and one for validation\n",
    "    singleFolder = (trainPerc == 0)\n",
    "    \n",
    "    # create output folders if they do not exist, if mosaic mode create train andn test subfolders\n",
    "    Path(outFolderRoot).mkdir(parents=True, exist_ok=True)\n",
    "    if not singleFolder :\n",
    "        Path(os.path.join(outFolderRoot,\"train\")).mkdir(parents=True, exist_ok=True)\n",
    "        Path(os.path.join(outFolderRoot,\"validation\")).mkdir(parents=True, exist_ok=True)\n",
    "        if doYOLO:\n",
    "            Path(os.path.join(outFolderRoot,\"train\",\"images\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"train\",\"masks\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"train\",\"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"validation\",\"images\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"validation\",\"masks\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"validation\",\"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "    else:    \n",
    "        if doYOLO:\n",
    "            Path(os.path.join(outFolderRoot,\"images\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"masks\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # add the name of the original image to every tile as a prefix (without extension)\n",
    "    outputPrefix = os.path.basename(mosFile)[:-4] \n",
    "    \n",
    "    # slice the three things and output\n",
    "    wSize = (slice,slice)\n",
    "    count = 0\n",
    "    for (x, y, window) in sliding_window(mosaic, stepSize = int(slice*0.8), windowSize = wSize ):\n",
    "        # get mask window\n",
    "        if window.shape[:2] == (slice,slice) :\n",
    "            labelW = labelIM[y:y + wSize[1], x:x + wSize[0]]\n",
    "            boxesW = filterBoxesWindow(boxes,y,y + wSize[1], x,x + wSize[0])\n",
    "            boxesWNorm = boxesWNorm = filterBoxesWindowNormalized(boxesNorm, y, y+slice, x, x+slice, full_w=mosaic.shape[1], full_h=mosaic.shape[0])\n",
    "\n",
    "            if verbose: print(boxesW)\n",
    "\n",
    "            # here we should probably add cleanUpMaskBlackPixels and maybe do it for YOLO too (in buildtrainvalidation?)\n",
    "            if len(boxesW) > 0:\n",
    "                # store them both, doing a randomDraw to see if they go to training or testing\n",
    "                outFolder = (outFolderRoot if singleFolder else \n",
    "                (os.path.join(outFolderRoot,\"train\") if randint(1,100) < trainPerc else os.path.join(outFolderRoot,\"validation\") ) )\n",
    "                if verbose: print(\"writing to \"+str(os.path.join(outFolder,\"Tile\"+str(count)+\".png\")))\n",
    "                if doYOLO:\n",
    "                    cv2.imwrite(os.path.join(outFolder, \"images\",outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\".png\"),window)\n",
    "                    cv2.imwrite(os.path.join(outFolder,\"masks\",outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\"MASK.png\"),labelW)\n",
    "                    boxCoordsToFile(os.path.join(outFolder,\"labels\",outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\".txt\"),boxesWNorm)\n",
    "                else:                    \n",
    "                    cv2.imwrite(os.path.join(outFolder,outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\".png\"),window)\n",
    "                    cv2.imwrite(os.path.join(outFolder,outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\"Labels.tif\"),labelW)\n",
    "                    boxCoordsToFile(os.path.join(outFolder,outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\"Boxes.txt\"),boxesW)\n",
    "                count+=1\n",
    "            else:\n",
    "                if verbose: print(\"no boxes here\")\n",
    "        else:\n",
    "            if verbose:  print(\"sliceFolder, non full window, ignoring\"+str(window.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5649f947-3143-43b4-9039-7f4fac1ab877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliceAndBoxNartData(prefix, outputFolder, sliceSize = 500, trainPerc = 0):\n",
    "    \"\"\"\n",
    "        receive one folder divided into prefi_image and \n",
    "        prefix_mask, one slice size in pixels\n",
    "        traverse all images in the \"image\" folder\n",
    "        make sure that they have a corresponding mask.\n",
    "        Slice and box them all into an outputFolder\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(prefix+\"_image\"):\n",
    "        for imageFile in files:\n",
    "            labelImageFile = os.path.join(prefix+\"_mask\",imageFile)\n",
    "            if os.path.isfile( labelImageFile ):\n",
    "                prepareData(labelImageFile,os.path.join(prefix+\"_image\",imageFile),outputFolder, 0, False, sliceSize, verbose = False)\n",
    "                prepareData(labelImageFile,os.path.join(prefix+\"_image\",imageFile),outputFolder, trainPerc, True, sliceSize, verbose = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ce615-412d-4b57-8c1b-15d12e92aa76",
   "metadata": {},
   "source": [
    "To call these two functions we need to define the path to our Data. The values included in the cell suppose that the structure mentioned above is followed. Change if your path is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d2d844-f2f3-4802-a8e4-01d198aa1118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main data Folder is/home/x/Experiments/DLTreeDetection/Data/NART\n"
     ]
    }
   ],
   "source": [
    "# Define where our data is, change if necessary, this cell must be run in every execution!\n",
    "\n",
    "# Define main data folder, supposed to be within the directory that contains the notebook then Data then NART.\n",
    "# Depending on how you are running the jupyter notebook, you may need to change the definition of this folder\n",
    "dataFolder = os.path.join(os.getcwd(), \"Data\", \"NART\") \n",
    "print(\"Main data Folder is\" +str(dataFolder))\n",
    "\n",
    "# Define input and output Folders for the training and testing data\n",
    "inputTrain = os.path.join(dataFolder,\"train\")\n",
    "trainData = os.path.join(dataFolder,\"processedTrain\")\n",
    "\n",
    "inputTest = os.path.join(dataFolder,\"test\")\n",
    "testData = os.path.join(dataFolder,\"processedTest\")\n",
    "\n",
    "# Most of the parameters of our experiments will be defined later, but in this case, we need to define \n",
    "# the number of pixels that our tiles will have on the side (sliceSize)\n",
    "# and the percentage of tiles in the training/validation set that will be used for training \n",
    "sliceSize = 500\n",
    "trainPercentage = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e7a38-1949-4e46-9e2f-acbc6b822a6d",
   "metadata": {},
   "source": [
    "We are now ready to translate our Data to a format that Pytorch can read:\n",
    "\n",
    "**you only need to run the following cell once, if you do not change the data do not run this cell again** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cefd22-065b-403d-878c-7d8081ee7b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all of our images to masks, box files and slices, if you did this before you do not need to run it again!\n",
    "# make one folder for training YOLO\n",
    "sliceAndBoxNartData(inputTrain,trainData, sliceSize = sliceSize, trainPerc = trainPercentage)\n",
    "# also make one folder for testing\n",
    "sliceAndBoxNartData(inputTest,testData, sliceSize = sliceSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c97d9e-73a3-4b73-9b31-51a8b06d04f2",
   "metadata": {},
   "source": [
    "Check that two new folders have been created inside of the main data folder with the names indicated in `trainData` and `testData`.\n",
    "\n",
    "These two new folders should have:\n",
    "- Three subfolders called `images`, `labels` and `masks`\n",
    "- Lots of files corresponding to the tiles. Each Tile will have an image file (for example `patch_2x0y0.png`) a text file with the information of its bounding boxes (for example `patch_2x0y0Boxes.txt`) and a labelimage (for example patch_2x0y0Labels.tif).\n",
    "\n",
    "Check that all of this worked without errors and we are now ready to run our...\n",
    "\n",
    "# Experiments\n",
    "\n",
    "These experiments are pretty complex and allow to train and test quite a lot of different networks. To begin with I focused only on the two networks that are more likely to give the best results. YOLO and MaskRCNN.\n",
    "\n",
    "Interesting Parameters:\n",
    "\n",
    "- \"Train\": Determines whether or not we want to retrain our model. Keep it `True` initially but if you only want to re-test your model, then you can set it to `False`.\n",
    "- \"ep\" contains the number of epochs that the model will be trained for. I recommend running an initial run with one epoch to test if your model works and then set it to whatever value you consider correct. So far a value of 200 is pretty commonly used.\n",
    "- \"batchSize\" is set to 4. This parameter is crucial in terms of speed, the bigger the better. However, if you make it too big then the GPU will run out of memory and the code will crash. I used size 8 in my large computer and every epoch took less than a minute. with size 1 every epoch was about 4 minutes. I recommend testing this extensively before launching into long experiments. The final stages of the code use a bit of extra GPU memory so do not cut it too short.  \n",
    "- \"doYolo\" and \"doPytorch\" control what network(s) we use. I recommend starting with the one-epoch experiment for both YOLO and Pytorch and then setting up individual experiment setting the other value to `False`. For the moment leave the \"doDETR\" parameter at `False`. This is another network that we can compare if we are interested in making a large comparison of models but the code has not been adapted properly and it is not likely to give better results.\n",
    "- the \"numClasses\" parameter is set to `3` and should reflect the number of different weed species present in the label data. If the number is correct you do not need to ever change it.\n",
    "- The \"Train_Perc\" and \"slice\" parameters refer to two previous parameters defined when we created the training data. If you want to change them, go back there, change them there and make sure to re-run the function to create the data (also erase the data previously processed to be sure). Do not change them here.\n",
    "- the    \"Pred_dir\" and \"outTEXT\" parameters contain the route to the output that the algorithm creates. The output for the Yolo algorithm is also stored in \"Train_res\" and \"Valid_res\". Change them if you prefer. **Make sure to check these out as they contain quite a lot of interesting information.** In particular, Yolo will dump lots of tile prediction files into the root of the predictions folder. MaskRCNN will create one subfolder for each parameter configuration. Inside of each subfolder you will find lots of files related to tile and one more subfolder called \"full\". Check this \"full\" folder for the masks of every full image and a \"Pretty\" image that shows the prediction boxes superimposed to the original image along with the category of each one. At this moment Yolo does not create these \"Pretty\" images, it is on the TODO list.\n",
    "- The code produces precision and recall values. **These are detection only.** so they do not concern themselves with whether the weed inside the box is very well segmented or not, only that it is properly found. Also, at this moment they do not check the category of the weed. For example, if a weed is detected correctly (it box is properly placed) but the category is wrong (1 is predicted instead of 2), at this moment this will be counted as a correct prediction. This will be improved in the near future, hopefully.\n",
    "  \n",
    "The rest of the parameters can be safely ignore for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba310360-8c43-4e57-8836-4dfb06863cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration of our experiments\n",
    "conf = {\n",
    "    \"Train\" : True,\n",
    "    \"ep\" : 1,\n",
    "    \"batchSize\": 4,\n",
    "    \"doYolo\" : True,\n",
    "    \"doPytorch\" : True,\n",
    "    \"doDETR\" : False,\n",
    "    \"numClasses\" : 3,\n",
    "    \"Train_Perc\" : trainPercentage,\n",
    "    \"slice\": sliceSize,\n",
    "    \"Pred_dir\" : \"Predictions\",\n",
    "    \"outTEXT\": \"results.txt\",\n",
    "    \"Train_res\": os.path.join(os.getcwd(), \"YOLOResults\"),\n",
    "    \"Valid_res\": os.path.join(os.getcwd(), \"YOLOResults\"),\n",
    "    \"Prep\" : False,\n",
    "    \"TV_dir\" : os.path.join(dataFolder,\"processedTrain\"),\n",
    "    \"Train_dir\" : \"train\",\n",
    "    \"Valid_dir\" : \"validation\",\n",
    "    \"Test_dir\" : os.path.join(dataFolder,\"processedTest\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c94ca-fdab-4533-9ae3-704854d555e4",
   "metadata": {},
   "source": [
    "Finally, the cell that actually runs the experiments. Let's start by leaving it as it is and then we can complicate things a bit by considering different parameters, other pytorch models, the DETR model. For now, let's make sure that it all works properly and that it can be run in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e024c811-e6ba-4d18-aad7-53a8401b4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n",
      "\n",
      "=== Running DETR Experiment ===\n",
      "Parameters: {'modelType': 'DETR', 'lr': 5e-06, 'batch_size': 8, 'predconf': 0.5, 'nms_iou': 0.5, 'max_detections': 50, 'resize': 800}\n",
      "[DETR] Testing params {'modelType': 'DETR', 'lr': 5e-06, 'batch_size': 8, 'predconf': 0.5, 'nms_iou': 0.5, 'max_detections': 50, 'resize': 800} with file DETR_expmodelTypeDETRlr5e-06batch_size8nms_iou0.5max_detections50resize800Epochs1.pth\n",
      "\n",
      "=== DATASET DEBUG ===\n",
      "Dataset length: 529\n",
      "\n",
      "=== DEBUG Sample 0 ===\n",
      "Image: patch_19x1200y400.png, Size: (500, 500)\n",
      "Number of raw boxes: 1\n",
      "First raw box: (1, 449, 0, 32, 25)\n",
      "All raw boxes: [(1, 449, 0, 32, 25)]\n",
      "Sample 0 - image shape: (500, 500, 3)\n",
      "Sample 0 - num annotations: 1\n",
      "First annotation: {'bbox': [449.0, 0.0, 32.0, 25.0], 'category_id': 0, 'iscrowd': 0, 'area': 800.0}\n",
      "===================\n",
      "\n",
      "[DETR] File: DETR_expmodelTypeDETRlr5e-06batch_size8nms_iou0.5max_detections50resize800Epochs1.pth, Train: True, Device: cpu, Epochs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/15/2025 15:19:47 - INFO - timm.models._builder -   Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "12/15/2025 15:19:47 - INFO - timm.models._hub -   [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "12/15/2025 15:19:47 - INFO - timm.models._builder -   Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DETR] Configuring model with num_labels=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/15/2025 15:19:48 - INFO - timm.models._builder -   Loading pretrained weights from Hugging Face hub (timm/resnet50.a1_in1k)\n",
      "12/15/2025 15:19:48 - INFO - timm.models._hub -   [timm/resnet50.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "12/15/2025 15:19:48 - INFO - timm.models._builder -   Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DETR] Initialized with pretrained weights (except class head)\n",
      "[DETR] LR schedule: drop by 0.1x every 5 epochs\n",
      "\n",
      "=== BATCH 0 DEBUG ===\n",
      "Number of images: 8\n",
      "Number of targets: 8\n",
      "First target keys: dict_keys(['annotations'])\n",
      "First target annotations: 24 annotations\n",
      "First annotation: {'bbox': [206.0, 0.0, 155.0, 79.0], 'category_id': 2, 'iscrowd': 0, 'area': 12245.0}\n",
      "===================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TRAINING DIAGNOSTIC\n",
      "======================================================================\n",
      "Config: num_labels=3, batch_size=8\n",
      "Pixel values: torch.Size([8, 3, 800, 800])\n",
      "First target: boxes=torch.Size([24, 4]), classes=[0, 1, 2]\n",
      "======================================================================\n",
      "\n",
      "\n",
      "=== DEBUG Sample 4 ===\n",
      "Image: patch_8x400y1600.png, Size: (500, 500)\n",
      "Number of raw boxes: 7\n",
      "First raw box: (1, 252, 106, 29, 27)\n",
      "All raw boxes: [(1, 252, 106, 29, 27), (1, 76, 170, 50, 42), (1, 42, 192, 20, 35), (1, 368, 199, 69, 55), (2, 486, 410, 14, 21), (1, 368, 458, 37, 37), (1, 313, 486, 30, 14)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m detr_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelType\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDETR\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5e-6\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m8\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredconf\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \n\u001b[1;32m      4\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnms_iou\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_detections\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresize\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m800\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoDETR\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run experiments\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mMODULARDLExperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpytorch_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetr_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Experiments/DLTreeDetection/experimentsUtils.py:295\u001b[0m, in \u001b[0;36mMODULARDLExperiment\u001b[0;34m(conf, yolo_params, pytorch_params, detr_params)\u001b[0m\n\u001b[1;32m    293\u001b[0m experiment \u001b[38;5;241m=\u001b[39m DETRExperiment(conf, device)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m metrics, train_time, test_time \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetr_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m experiment\u001b[38;5;241m.\u001b[39mwrite_results(detr_params, metrics, train_time, test_time)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDETR Results: Precision=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moprec\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morec\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Experiments/DLTreeDetection/experimentsUtils.py:202\u001b[0m, in \u001b[0;36mDETRExperiment.train_and_test\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m train_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: file_path,\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainAgain\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_again,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    199\u001b[0m }\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelType\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDETR\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 202\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_DETR\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetr_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDETR_exp_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelType\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEFDETR\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    204\u001b[0m     model \u001b[38;5;241m=\u001b[39m train_DeformableDETR(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconf, detr_dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDETR_exp_\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_params, file_path)\n",
      "File \u001b[0;32m~/Experiments/DLTreeDetection/train.py:366\u001b[0m, in \u001b[0;36mtrain_DETR\u001b[0;34m(conf, datasrc, prefix, params, file_path)\u001b[0m\n\u001b[1;32m    363\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;28;01mif\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28msum\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mloss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    365\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 366\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m10.0\u001b[39m)\n\u001b[1;32m    368\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/YOLO/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "yolo_params = {\"scale\": 0.3, \"mosaic\": 0.5} if conf[\"doYolo\"] else None\n",
    "pytorch_params = {\"modelType\": \"maskrcnn\", \"score\": 0.25, \"nms\": 0.5, \"predconf\": 0.7} if conf[\"doPytorch\"] else None\n",
    "detr_params = {\"modelType\": \"DETR\", \"lr\": 5e-6, \"batch_size\": 8, \"predconf\": 0.5, \n",
    "               \"nms_iou\": 0.5, \"max_detections\": 50, \"resize\": 800} if conf[\"doDETR\"] else None\n",
    "\n",
    "# Run experiments\n",
    "MODULARDLExperiment(conf, yolo_params, pytorch_params, detr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bfe1b5-2c6c-467d-9931-3f28155c08ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee4d12-8e8f-47ed-a02c-fbc4d416efcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
