{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d7db48-a626-4bea-b962-309327a74638",
   "metadata": {},
   "source": [
    "# Tree Detection with For Enshurin  Data using Deep Learning\n",
    "\n",
    "In this notebook we will use several Deep Learning networks to detect tree of several species in RGB mosaics.  \n",
    "\n",
    "The data was collected by Vlad\n",
    "\n",
    "## Prerequisites.\n",
    "\n",
    "- Due to the size of the networks, this code will not run in free google colab. Either paid colab or running in locally in a computer with GPU is necessary. In the following I will assume that the code is run in a local computer that has a GPU and is CUDA-capable.\n",
    "- The computer needs to have software to run Python and jupyter notebook. `Anaconda` is recommended. If you are reading this, you most likely have already solved this part.\n",
    "- Apart from this, you need to create a proper virtual environment to run the code into. I recomment creating the environment with `Anaconda` itself and then installing packages using `pip`. At the very least you will need to install:\n",
    "    - opencv (for general image handling)\n",
    "    - pytorch (for general DL computations and the following models fasterRCNN, convnextMaskRCNN, maskRCNN, FCOS, retinanet, SSD  \n",
    "    - ultralytics for YOLO\n",
    "    - Transformers for DETR\n",
    "    - several other smaller libraries for several dependencies.\n",
    "\n",
    "If you are reading this here it is likely that you already have this, but you should have donwloaded all the code from [this repository](https://github.com/nicill/DLTreeDetection). This notebook along with all `.py` files. Download them using git or just go to the webpage, click `code` and then `download zip`.\n",
    "\n",
    "Once you have the folder with the code and this notebook, you should copy the data into it. The data is accessible [her](https://www.dropbox.com/scl/fi/b53k3eojxdrf0g0q7q305/NART.zip?rlkey=o8v9fwuqtle8l14w58awjqjmm&st=8lqk3op9&dl=0). Make a `Data` subfolder into the \n",
    "folder that contains the code and decompress the `NART.zip` file into it. You will end up with a Structure like Data->NART->test_image to access the data (there are four folders in the lower level: test_image, test_mask, train_image and train_mask.\n",
    "\n",
    "\n",
    "# Getting Started\n",
    "\n",
    "Let's start by checking wheher or not the notebook has access to CUDA at this moment as it will determine whether or not you can run experiments on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05c6e88-b487-4214-966e-b2d9df2601e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports \n",
    "import configparser\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from random import randint\n",
    "\n",
    "from datasets import TDDataset\n",
    "\n",
    "from config import read_config\n",
    "from imageUtils import boxesFound,read_Color_Image,read_Binary_Mask,recoupMasks, sliding_window, boxCoordsToFile\n",
    "from train import train_YOLO,makeTrainYAML, get_transform, train_pytorchModel,train_DETR\n",
    "\n",
    "from dataHandling import computeBBfromLIEnshurin, filterBoxesWindow, filterBoxesWindowNormalized\n",
    "\n",
    "from predict import predict_yolo, predict_pytorch\n",
    "\n",
    "from experimentsUtils import MODULARDLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2812dcf-6b7d-44f6-856f-e069cf801f49",
   "metadata": {},
   "source": [
    "You should be able to run the cell above without errors. If you get any `cannot import` errors it means that the environment in which you are running your notebook does not have all the necessary packages, install them using pip.\n",
    "\n",
    "Next, let's make sure that CUDA is accessible. CUDA is the library that manages the GPU and is absolutely crucial to get the code running in a time frame that allows experiments to be run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dacb6da-0a2f-4ec4-9f95-bf69357a5bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS CUDA AVAILABLE???????????????????????\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# checking that CUDA is available\n",
    "print(\"IS CUDA AVAILABLE???????????????????????\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d59c7e-5851-4f59-89b0-7e6d3f33d7ec",
   "metadata": {},
   "source": [
    "You should see the `IS CUDA AVAILABLE???????????????????????` message in the first line and `True` in the second. If the second line prints `False` then you do not have CUDA properly configured. Do that before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecc367-b3c2-40f6-9d36-4ec476b1bba3",
   "metadata": {},
   "source": [
    "# Data preprocessing:\n",
    "\n",
    "Before we can run any experiment we need to process the data into a format that Python and its Deep Learning library `Pytroch` can use. In particular, here we well divide every image into a series of tiles of the size that we decide. Additionally, we will translate all the image files into PNG format and the label images into text files with list of bounding box coordinates. The following two cells cell contains the cdoe that does all of this. The first function processes one image along with its label image and the second one runs it over all the files in a folder. As two of the DL networks that we use need slightly different data formats, the second function calls the first one twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040d21b7-9838-434d-a09c-389edd610cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lIMfile, mosFile, outFolderRoot, trainPerc, doYOLO, slice, verbose = True):\n",
    "    \"\"\"\n",
    "        Given a mosaic and a label image, create bounding boxes for the label image\n",
    "        then tile mosaic and boxes and store in a folder\n",
    "        if trainPerc != 0 then divide into train and test folders\n",
    "    \"\"\"\n",
    "    labelIM = cv2.imread(lIMfile,cv2.IMREAD_UNCHANGED)\n",
    "    # check unique labels read    \n",
    "    print(np.unique(labelIM))\n",
    "    print(labelIM.shape)\n",
    "\n",
    "    mosaic = read_Color_Image(mosFile)\n",
    "    boxes, boxesNorm = computeBBfromLIEnshurin(labelIM)\n",
    "\n",
    "    # If the train percentage is  0 then we are making one single folder \n",
    "    # otherwise, make one folder for trainining and one for validation\n",
    "    singleFolder = (trainPerc == 0)\n",
    "    \n",
    "    # create output folders if they do not exist, if mosaic mode create train andn test subfolders\n",
    "    Path(outFolderRoot).mkdir(parents=True, exist_ok=True)\n",
    "    if not singleFolder :\n",
    "        Path(os.path.join(outFolderRoot,\"train\")).mkdir(parents=True, exist_ok=True)\n",
    "        Path(os.path.join(outFolderRoot,\"validation\")).mkdir(parents=True, exist_ok=True)\n",
    "        if doYOLO:\n",
    "            Path(os.path.join(outFolderRoot,\"train\",\"images\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"train\",\"masks\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"train\",\"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"validation\",\"images\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"validation\",\"masks\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"validation\",\"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "    else:    \n",
    "        if doYOLO:\n",
    "            Path(os.path.join(outFolderRoot,\"images\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"masks\")).mkdir(parents=True, exist_ok=True)\n",
    "            Path(os.path.join(outFolderRoot,\"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # add the name of the original image to every tile as a prefix (without extension)\n",
    "    outputPrefix = os.path.basename(mosFile)[:-4] \n",
    "    \n",
    "    # slice the three things and output\n",
    "    wSize = (slice,slice)\n",
    "    count = 0\n",
    "    for (x, y, window) in sliding_window(mosaic, stepSize = int(slice*0.8), windowSize = wSize ):\n",
    "        # get mask window\n",
    "        if window.shape[:2] == (slice,slice) :\n",
    "            labelW = labelIM[y:y + wSize[1], x:x + wSize[0]]\n",
    "            boxesW = filterBoxesWindow(boxes,y,y + wSize[1], x,x + wSize[0])\n",
    "            boxesWNorm = boxesWNorm = filterBoxesWindowNormalized(boxesNorm, y, y+slice, x, x+slice, full_w=mosaic.shape[1], full_h=mosaic.shape[0])\n",
    "\n",
    "            if verbose: print(boxesW)\n",
    "\n",
    "            # here we should probably add cleanUpMaskBlackPixels and maybe do it for YOLO too (in buildtrainvalidation?)\n",
    "            if len(boxesW) > 0:\n",
    "                # store them both, doing a randomDraw to see if they go to training or testing\n",
    "                outFolder = (outFolderRoot if singleFolder else \n",
    "                (os.path.join(outFolderRoot,\"train\") if randint(1,100) < trainPerc else os.path.join(outFolderRoot,\"validation\") ) )\n",
    "                if verbose: print(\"writing to \"+str(os.path.join(outFolder,\"Tile\"+str(count)+\".png\")))\n",
    "                if doYOLO:\n",
    "                    cv2.imwrite(os.path.join(outFolder, \"images\",outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\".png\"),window)\n",
    "                    cv2.imwrite(os.path.join(outFolder,\"masks\",outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\"MASK.png\"),labelW)\n",
    "                    boxCoordsToFile(os.path.join(outFolder,\"labels\",outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\".txt\"),boxesWNorm)\n",
    "                else:                    \n",
    "                    cv2.imwrite(os.path.join(outFolder,outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\".png\"),window)\n",
    "                    cv2.imwrite(os.path.join(outFolder,outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\"Labels.tif\"),labelW)\n",
    "                    boxCoordsToFile(os.path.join(outFolder,outputPrefix+\"x\"+str(x)+\"y\"+str(y)+\"Boxes.txt\"),boxesW)\n",
    "                count+=1\n",
    "            else:\n",
    "                if verbose: print(\"no boxes here\")\n",
    "        else:\n",
    "            if verbose:  print(\"sliceFolder, non full window, ignoring\"+str(window.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64e3d98f-1045-4444-a28a-3dcde09aed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliceAndBoxEnshurinData(inputFolder, labelImName, outputFolder, sliceSize = 500, trainPerc = 0):\n",
    "    \"\"\"\n",
    "        receive one folder divided into prefi_image and \n",
    "        prefix_mask, one slice size in pixels\n",
    "        traverse all images in the \"image\" folder\n",
    "        make sure that they have a corresponding mask.\n",
    "        Slice and box them all into an outputFolder\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(inputFolder):\n",
    "        labelImageFile = os.path.join(inputFolder,labelImName)\n",
    "        for imageFile in files:\n",
    "            if \"Label\" not in imageFile:\n",
    "                #prepareData(labelImageFile,os.path.join(inputFolder,imageFile),outputFolder, 0, False, sliceSize, verbose = False)\n",
    "                prepareData(labelImageFile,os.path.join(inputFolder,imageFile),outputFolder, 0, False, sliceSize, verbose = False)\n",
    "                prepareData(labelImageFile,os.path.join(inputFolder,imageFile),outputFolder, trainPerc, True, sliceSize, verbose = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ce615-412d-4b57-8c1b-15d12e92aa76",
   "metadata": {},
   "source": [
    "To call these two functions we need to define the path to our Data. The values included in the cell suppose that the structure mentioned above is followed. Change if your path is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d2d844-f2f3-4802-a8e4-01d198aa1118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main data Folder is/home/yago/experiments/DLTreeDetection/Data/VladEnshurin/original\n"
     ]
    }
   ],
   "source": [
    "# Define where our data is, change if necessary, this cell must be run in every execution!\n",
    "\n",
    "# Define main data folder, supposed to be within the directory that contains the notebook then Data then NART.\n",
    "# Depending on how you are running the jupyter notebook, you may need to change the definition of this folder\n",
    "dataFolder = os.path.join(os.getcwd(), \"Data\", \"VladEnshurin\",\"original\") \n",
    "print(\"Main data Folder is\" +str(dataFolder))\n",
    "\n",
    "# Define input and output Folders for the training and testing data\n",
    "inputTrain = os.path.join(dataFolder,\"train\")\n",
    "trainData = os.path.join(dataFolder,\"processedTrain\")\n",
    "\n",
    "inputTest = os.path.join(dataFolder,\"test\")\n",
    "testData = os.path.join(dataFolder,\"processedTest\")\n",
    "\n",
    "# Most of the parameters of our experiments will be defined later, but in this case, we need to define \n",
    "# the number of pixels that our tiles will have on the side (sliceSize)\n",
    "# and the percentage of tiles in the training/validation set that will be used for training \n",
    "sliceSize = 1024\n",
    "trainPercentage = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e7a38-1949-4e46-9e2f-acbc6b822a6d",
   "metadata": {},
   "source": [
    "We are now ready to translate our Data to a format that Pytorch can read:\n",
    "\n",
    "**you only need to run the following cell once, if you do not change the data do not run this cell again** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4cefd22-065b-403d-878c-7d8081ee7b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "(33045, 30765)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "(33045, 30765)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "(33045, 30765)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "(33045, 30765)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "(33045, 30765)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "(33045, 30765)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "(33045, 30765)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "(33045, 30765)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "(33045, 30765)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "(33045, 30765)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14]\n",
      "(13287, 20977)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14]\n",
      "(13287, 20977)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14]\n",
      "(13287, 20977)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14]\n",
      "(13287, 20977)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14]\n",
      "(13287, 20977)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14]\n",
      "(13287, 20977)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14]\n",
      "(13287, 20977)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14]\n",
      "(13287, 20977)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14]\n",
      "(13287, 20977)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14]\n",
      "(13287, 20977)\n"
     ]
    }
   ],
   "source": [
    "# convert all of our images to masks, box files and slices, if you did this before you do not need to run it again!\n",
    "# make one folder for training YOLO\n",
    "sliceAndBoxEnshurinData(inputTrain,\"Label_14classes_train.tif\",trainData, sliceSize = sliceSize, trainPerc = trainPercentage)\n",
    "# also make one folder for testing\n",
    "sliceAndBoxEnshurinData(inputTest, \"Label_14classes_test.tif\", testData, sliceSize = sliceSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c97d9e-73a3-4b73-9b31-51a8b06d04f2",
   "metadata": {},
   "source": [
    "Check that two new folders have been created inside of the main data folder with the names indicated in `trainData` and `testData`.\n",
    "\n",
    "These two new folders should have:\n",
    "- Three subfolders called `images`, `labels` and `masks`\n",
    "- Lots of files corresponding to the tiles. Each Tile will have an image file (for example `patch_2x0y0.png`) a text file with the information of its bounding boxes (for example `patch_2x0y0Boxes.txt`) and a labelimage (for example patch_2x0y0Labels.tif).\n",
    "\n",
    "Check that all of this worked without errors and we are now ready to run our...\n",
    "\n",
    "# Experiments\n",
    "\n",
    "These experiments are pretty complex and allow to train and test quite a lot of different networks. To begin with I focused only on the two networks that are more likely to give the best results. YOLO and MaskRCNN.\n",
    "\n",
    "Interesting Parameters:\n",
    "\n",
    "- \"Train\": Determines whether or not we want to retrain our model. Keep it `True` initially but if you only want to re-test your model, then you can set it to `False`.\n",
    "- \"ep\" contains the number of epochs that the model will be trained for. I recommend running an initial run with one epoch to test if your model works and then set it to whatever value you consider correct. So far a value of 200 is pretty commonly used.\n",
    "- \"doYolo\" and \"doPytorch\" control what network(s) we use. I recommend starting with the one-epoch experiment for both YOLO and Pytorch and then setting up individual experiment setting the other value to `False`. For the moment leave the \"doDETR\" parameter at `False`. This is another network that we can compare if we are interested in making a large comparison of models but the code has not been adapted properly and it is not likely to give better results.\n",
    "- the \"numClasses\" parameter is set to `3` and should reflect the number of different weed species present in the label data. If the number is correct you do not need to ever change it.\n",
    "- The \"Train_Perc\" and \"slice\" parameters refer to two previous parameters defined when we created the training data. If you want to change them, go back there, change them there and make sure to re-run the function to create the data (also erase the data previously processed to be sure). Do not change them here.\n",
    "- the    \"Pred_dir\" and \"outTEXT\" parameters contain the route to the output that the algorithm creates, make sure to check them out as they contain quite a lot of interesting information. The output for the Yolo algorithm is stored in \"Train_res\" and \"Valid_res\". Change them if you prefer.\n",
    "  \n",
    "The rest of the parameters can be safely ignore for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba310360-8c43-4e57-8836-4dfb06863cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration of our experiments\n",
    "conf = {\n",
    "    \"Train\" : True,\n",
    "    \"ep\" : 2,\n",
    "    \"batchSize\": 8,\n",
    "    \"doYolo\" : False,\n",
    "    \"doPytorch\" : True,\n",
    "    \"doDETR\" : False,\n",
    "    \"numClasses\" : 14,\n",
    "    \"Train_Perc\" : trainPercentage,\n",
    "    \"slice\": sliceSize,\n",
    "    \"Pred_dir\" : \"/home/yago/Yago Lab Dropbox/Meetings/ongoingForestryExps/DLtreeDetection/predictions\",\n",
    "    \"outTEXT\": \"results.txt\",\n",
    "    \"Train_res\": os.path.join(os.getcwd(), \"YOLOResults\"),\n",
    "    \"Valid_res\": os.path.join(os.getcwd(), \"YOLOResults\"),\n",
    "    \"Prep\" : False,\n",
    "    \"TV_dir\" : os.path.join(dataFolder,\"processedTrain\"),\n",
    "    \"Train_dir\" : \"train\",\n",
    "    \"Valid_dir\" : \"validation\",\n",
    "    \"Test_dir\" : os.path.join(dataFolder,\"processedTest\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c94ca-fdab-4533-9ae3-704854d555e4",
   "metadata": {},
   "source": [
    "Finally, the cell that actually runs the experiments. Let's start by leaving it as it is and then we can complicate things a bit by considering different parameters, other pytorch models, the DETR model. For now, let's make sure that it all works properly and that it can be run in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e024c811-e6ba-4d18-aad7-53a8401b4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n",
      "\n",
      "=== Running PyTorch Model Experiment ===\n",
      "Parameters: {'modelType': 'maskrcnn', 'score': 0.25, 'nms': 0.5, 'predconf': 0.7}\n",
      "Train dataset length: 3620\n",
      "Testing params {'modelType': 'maskrcnn', 'score': 0.25, 'nms': 0.5, 'predconf': 0.7} with file expmodelTypemaskrcnnscore0.25nms0.5Epochs2.pth\n",
      "Inside Pytorch training Training Dataset Length 3620\n",
      "not training again\n",
      "Testing Dataset Length 795\n",
      "average Precision (centroids) 0.6125730994152047\n",
      "average Recall (centroids) 0.3684941520467837\n",
      "[0.6666666666666666, 1.0, 0, 0, 1.0, 1.0, 0, 0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0.6666666666666666, 0.5, 0, 1.0, 0, 0.25, 1.0, 1.0, 0.75, 0, 1.0, 0.0, 0.5, 0, 1.0, 1.0, 0.5, 0, 1.0, 1.0, 1.0, 0, 1.0, 0.6666666666666666, 0.25, 0.5, 0, 0, 1.0, 0.0, 0.6666666666666666, 1.0, 0.5, 1.0, 0, 0, 0, 0.3333333333333333, 0, 1.0, 0.3333333333333333, 1.0, 0.0, 0, 1.0, 0, 0, 1.0, 0, 0, 0.5, 1.0, 0.6666666666666666, 0, 0.5, 1.0, 0, 0, 0.0, 1.0, 0, 0, 1.0, 1.0, 0.6666666666666666, 0, 1.0, 1.0, 0.5, 0, 1.0, 1.0, 1.0, 1.0, 0, 1.0, 1.0, 0.75, 0.5, 0.6666666666666666, 1.0, 0.5, 0.0, 0, 1.0, 1.0, 0.5, 1.0, 1.0, 0.3333333333333333, 1.0, 0, 0.25, 0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.6666666666666666, 0.5, 0, 0, 0.0, 0, 1.0, 0, 0, 1.0, 0, 0, 0.5, 0.5, 1.0, 1.0, 1.0, 0, 1.0, 0.0, 1.0, 1.0, 0, 0, 0, 1.0, 1.0, 1.0, 1.0, 1.0, 0, 0.5, 0, 0, 0, 0.5, 0, 0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.6666666666666666, 1.0, 0, 1.0, 1.0, 0, 0, 0, 0.5, 0, 0.0, 0, 0.5, 0.0, 0, 1.0, 0.3333333333333333, 1.0, 1.0, 0.5, 0.6666666666666666, 0.0, 0, 0.5, 1.0, 0, 0, 0, 0.0, 0, 1.0, 0, 0, 0, 1.0, 1.0, 0.6666666666666666, 0.5, 1.0, 1.0, 0.3333333333333333, 0.5, 0, 0.5, 0, 0.6666666666666666, 1.0, 1.0, 0, 0.5, 0, 1.0, 0.0, 0, 0.0, 1.0, 1.0, 1.0, 0, 1.0, 1.0, 0, 0.5, 1.0, 0.0, 1.0, 1.0, 0, 0, 0.6666666666666666, 0, 0.6666666666666666, 0.5, 1.0, 1.0, 0, 0.0, 1.0, 0, 0.0, 0, 1.0, 1.0, 0, 0, 0, 1.0, 1.0, 0, 1.0, 0.0, 0, 0.5, 1.0, 0, 1.0, 0.6666666666666666, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 1.0, 0.6666666666666666, 0.75, 0, 0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0, 1.0, 0, 0, 0.5, 0, 0, 1.0, 1.0, 0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 0, 0, 0.5, 0.0, 1.0, 0.5, 1.0, 1.0, 0, 0.0, 0, 1.0, 1.0, 0, 0, 1.0, 1.0, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0, 1.0, 0, 1.0, 0, 1.0, 0, 0, 1.0, 0, 1.0, 0, 1.0, 0, 0.5, 0.6666666666666666, 0.5, 0.0, 0.5, 1.0, 1.0, 0, 0.6666666666666666, 0, 0.5, 0.0, 0.0, 0.5, 1.0, 0, 1.0, 1.0, 1.0, 0, 1.0, 1.0, 1.0, 0, 0, 1.0, 0, 0.5, 0, 1.0, 1.0, 1.0, 0.5, 0.0, 0, 0.0, 0.0, 0.5, 0, 0, 0.5, 0.0, 0, 1.0, 1.0, 0, 0.0, 0.5, 0.5, 0, 0, 1.0, 1.0, 1.0, 0, 1.0, 0, 0, 0.0, 1.0, 1.0, 0.5, 0, 0, 1.0, 0.6666666666666666, 0.3333333333333333, 0, 1.0, 1.0, 1.0, 0, 0.6666666666666666, 0.6666666666666666, 0.5, 1.0, 0, 0, 0.5, 0, 1.0, 0, 0, 0.6666666666666666, 0, 0.0, 1.0, 0, 0, 0, 0, 1.0, 0, 0.6666666666666666, 0.75, 1.0, 0, 1.0, 0, 0, 0.6666666666666666, 1.0, 0, 0, 0.5, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 0, 0.6666666666666666, 0, 1.0, 0.6666666666666666, 1.0, 0.6666666666666666, 0.0, 0, 0.3333333333333333, 1.0, 0, 1.0, 1.0, 0, 1.0, 1.0, 0.5, 1.0, 0, 1.0, 1.0, 0, 0.3333333333333333, 1.0, 1.0, 0, 0, 0, 0.0, 0.6666666666666666, 1.0, 0, 1.0, 1.0, 0, 0, 0.75, 0, 0.5, 0, 0, 1.0, 1.0, 1.0, 0, 0, 0.0, 1.0, 0, 0.5, 0, 0, 0, 0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0, 0.75, 0, 1.0, 0, 1.0, 1.0, 1.0, 0.6666666666666666, 1.0, 0.5, 0, 0, 0, 1.0, 1.0, 0, 1.0, 0, 0.5, 0.0, 0, 1.0, 1.0, 1.0, 0, 0, 0, 1.0, 0.0, 1.0, 0.5, 0.5, 0.5, 0, 1.0, 1.0, 0.5, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0, 1.0, 1.0, 0.0, 0, 1.0, 0, 1.0, 1.0, 0.0]\n",
      "[0.4, 0.25, 0.0, 0.0, 0.3333333333333333, 0.5, 0.0, 0.0, 0.5, 0.5, 0.3333333333333333, 1.0, 1.0, 0.5, 1.0, 0.5, 0.5, 0.0, 0.3333333333333333, 0.0, 0.16666666666666666, 1.0, 0.5, 0.6, 0.0, 1.0, 0.0, 0.25, 0.0, 0.2, 0.5, 0.3333333333333333, 0.0, 0.2, 1.0, 0.5, 0.0, 0.6666666666666666, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 1.0, 1.0, 0.5, 0.2, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.16666666666666666, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.2, 1.0, 0.6666666666666666, 0.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.5, 0.0, 0.6666666666666666, 0.3333333333333333, 0.5, 0.0, 0.4, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.0, 1.0, 0.5, 0.5, 1.0, 0.25, 0.2222222222222222, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.5, 0.5, 0.25, 0.3333333333333333, 0.5, 0.0, 0.25, 0.0, 0.3333333333333333, 1.0, 0.6666666666666666, 0.4, 0.1111111111111111, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.16666666666666666, 0.5, 1.0, 0.5, 0.6, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.5, 0.0, 0.0, 0.0, 0.375, 0.25, 0.5, 0.3333333333333333, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333, 1.0, 0.5, 0.6666666666666666, 0.3333333333333333, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.3333333333333333, 0.2, 1.0, 0.16666666666666666, 0.5, 0.6666666666666666, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.5, 0.6666666666666666, 1.0, 0.25, 0.5, 0.3333333333333333, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 1.0, 0.5, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 0.0, 1.0, 0.5, 0.4, 0.0, 0.5, 0.3333333333333333, 0.0, 0.25, 0.6, 0.0, 0.5, 0.5, 0.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.25, 0.25, 0.0, 0.0, 0.0, 1.0, 0.3333333333333333, 0.0, 0.5, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.5, 0.6666666666666666, 1.0, 0.0, 0.0, 0.2, 0.3333333333333333, 0.0, 0.3333333333333333, 0.3333333333333333, 0.2, 0.4, 0.3333333333333333, 0.0, 0.5, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.25, 0.2222222222222222, 0.0, 1.0, 0.3333333333333333, 0.0, 0.5, 0.5, 0.5, 1.0, 0.3333333333333333, 1.0, 0.6, 0.4, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.5, 0.5, 0.6666666666666666, 0.5, 0.0, 0.0, 0.0, 0.6, 0.3333333333333333, 0.0, 0.0, 0.5, 0.6666666666666666, 0.6666666666666666, 1.0, 0.2, 1.0, 0.0, 0.0, 0.5, 0.5, 0.6666666666666666, 0.6666666666666666, 0.25, 0.5, 0.0, 0.3333333333333333, 0.0, 0.375, 0.0, 0.3333333333333333, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.3333333333333333, 0.4, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.4, 0.0, 0.3333333333333333, 0.0, 0.0, 0.5, 0.5, 0.0, 0.6666666666666666, 0.5, 0.6, 0.0, 0.2, 1.0, 0.5, 0.0, 0.0, 0.3333333333333333, 0.0, 0.75, 0.0, 1.0, 1.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.125, 0.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.2, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 1.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.5, 0.5, 0.4, 0.0, 0.0, 0.5, 0.4, 0.5, 0.0, 0.6666666666666666, 0.3333333333333333, 0.5, 0.0, 0.3333333333333333, 0.5, 0.2, 0.5, 0.0, 0.0, 0.5, 0.0, 0.2, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25, 1.0, 1.0, 0.0, 0.3333333333333333, 0.0, 0.6, 0.25, 1.0, 0.3333333333333333, 0.0, 0.0, 0.1111111111111111, 1.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.0, 0.5, 1.0, 0.3333333333333333, 1.0, 0.0, 0.5, 0.4, 0.0, 1.0, 0.6666666666666666, 0.16666666666666666, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.5, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 1.0, 0.0, 0.25, 0.0, 0.0, 0.5, 0.5, 0.2, 0.0, 0.0, 0.0, 0.5, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.25, 0.0, 0.25, 0.0, 0.16666666666666666, 0.0, 0.6, 0.0, 0.3333333333333333, 0.0, 0.5, 1.0, 0.3333333333333333, 0.6666666666666666, 0.5, 0.25, 0.0, 0.0, 0.0, 0.6666666666666666, 0.25, 0.0, 0.5, 0.0, 0.25, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.2, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.125, 0.3333333333333333, 0.3333333333333333, 0.0, 0.3333333333333333, 0.25, 0.5, 0.5, 0.0, 0.6666666666666666, 0.3333333333333333, 1.0, 0.6666666666666666, 0.5, 0.3333333333333333, 0.2222222222222222, 0.5, 1.0, 0.0, 0.5, 0.5, 0.0, 0.0, 0.8, 0.0, 0.5, 0.5, 0.0]\n",
      "average Precision (overlap) 0.4972222222222222\n",
      "average Recall (overlap) 0.31734892787524366\n",
      "global Precision (overlap) 0.711764705882353\n",
      "global Recall (overlap) 0.30440251572327043\n",
      "PyTorch Results: Precision=0.712, Recall=0.304\n",
      "\n",
      "=== All Experiments Complete ===\n"
     ]
    }
   ],
   "source": [
    "yolo_params = {\"scale\": 0.3, \"mosaic\": 0.5} if conf[\"doYolo\"] else None\n",
    "pytorch_params = {\"modelType\": \"maskrcnn\", \"score\": 0.25, \"nms\": 0.5, \"predconf\": 0.7} if conf[\"doPytorch\"] else None\n",
    "detr_params = {\"modelType\": \"DETR\", \"lr\": 5e-6, \"batch_size\": 8, \"predconf\": 0.5, \n",
    "               \"nms_iou\": 0.5, \"max_detections\": 50, \"resize\": 800} if conf[\"doDETR\"] else None\n",
    "\n",
    "# Run experiments\n",
    "MODULARDLExperiment(conf, yolo_params, pytorch_params, detr_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bfe1b5-2c6c-467d-9931-3f28155c08ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b547a-098f-4e86-8ffd-df2762221547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
